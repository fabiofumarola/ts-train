{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "from typing import List, Tuple, Union, Dict\n",
    "from pyspark.sql.dataframe import DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/24 10:26:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/07/24 10:27:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark Experimentation\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea la sessione di Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Definisci lo schema del DataFrame di esempio\n",
    "schema = [\"id\", \"timestamp\", \"numerical_1\", \"numerical_2\", \"categorical_feature_1\", \"categorical_feature_2\"]\n",
    "\n",
    "# Crea il DataFrame di esempio\n",
    "data_df = spark.createDataFrame([\n",
    "    (1, \"2023-06-01\", 1, 2, \"pasta\", \"bitcoin\"),\n",
    "    (1, \"2023-06-01\", 2, 3, \"pasta\", \"cash\"),\n",
    "    (1, \"2023-06-01\", 4, 1, \"spezie\", \"bancomat\"),\n",
    "    (1, \"2023-06-01\", 6, 1, \"spazzolini\", \"bancomat\"),\n",
    "    (1, \"2023-06-02\", 7, 6, \"pasta\", \"bancomat\"),\n",
    "    (1, \"2023-06-02\", 7, 6, \"spezie\", \"bancomat\"),\n",
    "    (1, \"2023-06-06\", 4, 2, \"pasta\", \"cash\"),\n",
    "    (2, \"2023-06-03\", 10, 12, \"pasta\", \"cash\"),\n",
    "    (2, \"2023-06-03\", 13, 15, \"spazzolini\", \"cash\"),\n",
    "    (2, \"2023-06-03\", 1, 15, \"spazzolini\", \"cash\"),\n",
    "], schema=schema)\n",
    "\n",
    "\n",
    "# Converti la colonna \"timestamp\" in formato data\n",
    "data_df = data_df.withColumn(\"timestamp\", F.to_timestamp(F.col(\"timestamp\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- numerical_1: long (nullable = true)\n",
      " |-- numerical_2: long (nullable = true)\n",
      " |-- categorical_feature_1: string (nullable = true)\n",
      " |-- categorical_feature_2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+-----------+---------------------+---------------------+\n",
      "| id|          timestamp|numerical_1|numerical_2|categorical_feature_1|categorical_feature_2|\n",
      "+---+-------------------+-----------+-----------+---------------------+---------------------+\n",
      "|  1|2023-06-01 00:00:00|          1|          2|                pasta|              bitcoin|\n",
      "|  1|2023-06-01 00:00:00|          2|          3|                pasta|                 cash|\n",
      "|  1|2023-06-01 00:00:00|          4|          1|               spezie|             bancomat|\n",
      "|  1|2023-06-01 00:00:00|          6|          1|           spazzolini|             bancomat|\n",
      "|  1|2023-06-02 00:00:00|          7|          6|                pasta|             bancomat|\n",
      "|  1|2023-06-02 00:00:00|          7|          6|               spezie|             bancomat|\n",
      "|  1|2023-06-06 00:00:00|          4|          2|                pasta|                 cash|\n",
      "|  2|2023-06-03 00:00:00|         10|         12|                pasta|                 cash|\n",
      "|  2|2023-06-03 00:00:00|         13|         15|           spazzolini|                 cash|\n",
      "|  2|2023-06-03 00:00:00|          1|         15|           spazzolini|                 cash|\n",
      "+---+-------------------+-----------+-----------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP1: Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+-----------+---------------------+---------------------+------------------------------------------+\n",
      "|id |timestamp          |numerical_1|numerical_2|categorical_feature_1|categorical_feature_2|bucket                                    |\n",
      "+---+-------------------+-----------+-----------+---------------------+---------------------+------------------------------------------+\n",
      "|1  |2023-06-01 00:00:00|1          |2          |pasta                |bitcoin              |{2023-05-31 00:00:00, 2023-06-02 00:00:00}|\n",
      "|1  |2023-06-01 00:00:00|2          |3          |pasta                |cash                 |{2023-05-31 00:00:00, 2023-06-02 00:00:00}|\n",
      "|1  |2023-06-01 00:00:00|4          |1          |spezie               |bancomat             |{2023-05-31 00:00:00, 2023-06-02 00:00:00}|\n",
      "|1  |2023-06-01 00:00:00|6          |1          |spazzolini           |bancomat             |{2023-05-31 00:00:00, 2023-06-02 00:00:00}|\n",
      "|1  |2023-06-02 00:00:00|7          |6          |pasta                |bancomat             |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|\n",
      "|1  |2023-06-02 00:00:00|7          |6          |spezie               |bancomat             |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|\n",
      "|1  |2023-06-06 00:00:00|4          |2          |pasta                |cash                 |{2023-06-06 00:00:00, 2023-06-08 00:00:00}|\n",
      "|2  |2023-06-03 00:00:00|10         |12         |pasta                |cash                 |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|\n",
      "|2  |2023-06-03 00:00:00|13         |15         |spazzolini           |cash                 |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|\n",
      "|2  |2023-06-03 00:00:00|1          |15         |spazzolini           |cash                 |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|\n",
      "+---+-------------------+-----------+-----------+---------------------+---------------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "utf_shift_hours = 2\n",
    "windows_size = \"2 days\"\n",
    "\n",
    "window_column = F.window(timeColumn=F.col(\"timestamp\"), windowDuration=windows_size, startTime=f\"-{utf_shift_hours} hours\")\n",
    "data_df = data_df.withColumn(\"bucket\", window_column)\n",
    "data_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP2: Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########  Generazione di tutte le aggregazioni \n",
    "# genero il prodotto cartesiano di tutte le possibili funzioni di aggregazioni su tutte le possibili colonne numeriche\n",
    "# lista di tutti dizionari di aggregazioni possibili\n",
    "def _all_aggregation_combination(numerical_col_name: list[str], aggr_functions: list[str])->list[dict[str,str]]:\n",
    "    all_aggregations = []\n",
    "    for col in numerical_col_name:\n",
    "        for func in aggr_functions:\n",
    "            all_aggregations.append({col: func})\n",
    "\n",
    "    return all_aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########  Fase di grouping \n",
    "# Algoritmo:\n",
    "# Definisci la lista di colonne ([id_columns] + windows_column)\n",
    "def _grouping(data_df: DataFrame, identifier_cols_name: list[str]) -> DataFrame:\n",
    "    grouped_columns = [F.col(col) for col in identifier_cols_name] \n",
    "    grouped_df = data_df.groupBy(*grouped_columns)\n",
    "    return grouped_df\n",
    "    #capire se ha senso cachare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pivot senza filtri\n",
      "+---+-----+----------+------+\n",
      "| id|pasta|spazzolini|spezie|\n",
      "+---+-----+----------+------+\n",
      "|  1|   14|         6|    11|\n",
      "|  2|   10|        14|  null|\n",
      "+---+-----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########  Fase di Aggregation\n",
    "\n",
    "def _pivoting(grouped_df: DataFrame, filter: list[tuple[str,list[str]]], aggregation: dict[str,str]) -> DataFrame:\n",
    "    cat_var_name,cat_options = filter[0]\n",
    "    if len(cat_options) == 0:\n",
    "        print(\"pivot senza filtri\")\n",
    "        pivoted_df = grouped_df.pivot(cat_var_name).agg(aggregation)\n",
    "    else: \n",
    "        print(\"pivot con filtri di opzioni\")\n",
    "        pivoted_df = grouped_df.pivot(cat_var_name,cat_options).agg(aggregation)\n",
    "    return pivoted_df \n",
    "\n",
    "#schema = [\"id\", \"timestamp\", \"numerical_1\", \"numerical_2\", \"categorical_feature_1\", \"categorical_feature_2\"]\n",
    "_pivoting(data_df.groupBy(\"id\"),[(\"categorical_feature_1\",[])],{\"numerical_1\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sum_of_importo_by_cat_lv0_and_by_cat_lv1_(pasta_penne)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_column_name(operation: str, filters: list[tuple[str,list[str]]], numerical_col_name: str ) -> str:\n",
    "    new_col_name = f\"{operation}_of_{numerical_col_name}\"\n",
    "    for filter in filters:\n",
    "        create_column_name, categotrical_options = filter\n",
    "        if len(create_column_name) >= 0:\n",
    "            new_col_name += f\"_by_{create_column_name}\"\n",
    "        if len(categotrical_options) > 0:\n",
    "            new_col_name += f\"_({'_'.join(categotrical_options)})\"\n",
    "        new_col_name += f\"_and\"\n",
    "    new_col_name = new_col_name[:-4] # remove last \"_and_\"\n",
    "    return new_col_name\n",
    "\n",
    "\n",
    "create_column_name(\"sum\",[(\"cat_lv0\",[]), (\"cat_lv1\", [\"pasta\", \"penne\"])],\"importo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VERSIONE SU SINGOLO UTENTE\n",
    "def _selecting(\n",
    "        self,\n",
    "        df: DataFrame,\n",
    "        filters: list[tuple[str, list[str]]],\n",
    "        aggregation: dict[str, str],\n",
    "    ) -> DataFrame:\n",
    "        # Filtra le righe che soddisfano le condizioni desiderate\n",
    "        filter_conditions = []\n",
    "        for single_filter in filters:\n",
    "            filter_column, filter_options = single_filter\n",
    "            # controllare se filter_options non sia vuota\n",
    "            filter_conditions.append(F.col(filter_column).isin(filter_options))\n",
    "\n",
    "        # Combina tutti i filtri in una unica espressione logica utilizzando l'operatore logico AND\n",
    "        # lambda lambda f1, f2: f1 & f2 -> funzione anonima che prende un due filtri e una condizione e ritorna l'unione dei due filtri secondo la condizione data\n",
    "        # reduce(funzione, lista) -> applica la funzione a tutti gli elementi della lista e ritorna il risultato\n",
    "        # Applica il filtro complessivo una volta sola\n",
    "        filter_expression = reduce(lambda f1, f2: f1 & f2, filter_conditions)\n",
    "\n",
    "        filtered_df = df.filter(filter_expression)\n",
    "        return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+-----------+---------------------+---------------------+--------------------+\n",
      "| id|          timestamp|numerical_1|numerical_2|categorical_feature_1|categorical_feature_2|              bucket|\n",
      "+---+-------------------+-----------+-----------+---------------------+---------------------+--------------------+\n",
      "|  1|2023-06-01 00:00:00|          1|          2|                pasta|              bitcoin|{2023-05-31 00:00...|\n",
      "|  1|2023-06-01 00:00:00|          2|          3|                pasta|                 cash|{2023-05-31 00:00...|\n",
      "|  1|2023-06-01 00:00:00|          4|          1|               spezie|             bancomat|{2023-05-31 00:00...|\n",
      "|  1|2023-06-01 00:00:00|          6|          1|           spazzolini|             bancomat|{2023-05-31 00:00...|\n",
      "|  1|2023-06-02 00:00:00|          7|          6|                pasta|             bancomat|{2023-06-02 00:00...|\n",
      "|  1|2023-06-02 00:00:00|          7|          6|               spezie|             bancomat|{2023-06-02 00:00...|\n",
      "|  1|2023-06-06 00:00:00|          4|          2|                pasta|                 cash|{2023-06-06 00:00...|\n",
      "|  2|2023-06-03 00:00:00|         10|         12|                pasta|                 cash|{2023-06-02 00:00...|\n",
      "|  2|2023-06-03 00:00:00|         13|         15|           spazzolini|                 cash|{2023-06-02 00:00...|\n",
      "|  2|2023-06-03 00:00:00|          1|         15|           spazzolini|                 cash|{2023-06-02 00:00...|\n",
      "+---+-------------------+-----------+-----------+---------------------+---------------------+--------------------+\n",
      "\n",
      "+---+--------------------+---------------------------------------------------------------+\n",
      "| id|              bucket|sum_of_numerical_1_by_categorical_feature_1_(spazzolini_spezie)|\n",
      "+---+--------------------+---------------------------------------------------------------+\n",
      "|  1|{2023-05-31 00:00...|                                                             10|\n",
      "|  1|{2023-06-02 00:00...|                                                              7|\n",
      "|  2|{2023-06-02 00:00...|                                                             14|\n",
      "+---+--------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### VERSIONE FUNZIONANTE MA POCO EFFICIENTE\n",
    "\n",
    "def _selecting(df: DataFrame, identifier_cols_name: List[str], filters: List[Tuple[str, List[str]]], aggregation: Dict[str, str]) -> DataFrame:\n",
    "    # Applicare i filtri alle variabili categoriche\n",
    "    # Raggruppa i dati in base all'ID utente\n",
    "    \n",
    "    unique_ids = data_df.select(\"id\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    list_of_dataframes = []\n",
    "    for uid in unique_ids:\n",
    "        df= data_df.filter(data_df[\"id\"] == uid)\n",
    "        \n",
    "        filter_conditions = []\n",
    "        for single_filter in filters:\n",
    "            filter_column, filter_options = single_filter\n",
    "            # controllare se filter_options non sia vuota\n",
    "            filter_conditions.append(F.col(filter_column).isin(filter_options))\n",
    "    \n",
    "        filter_expression = reduce(lambda f1, f2: f1 & f2, filter_conditions)\n",
    "        filtered_df = df.filter(filter_expression)\n",
    "        \n",
    "        # Aggregare i dati per ottenere la somma su ogni finestra\n",
    "        result_df = filtered_df.groupBy(*identifier_cols_name).agg(aggregation)\n",
    "        \n",
    "\n",
    "        # Rinominare le colonne risultanti usando la funzione di utilità\n",
    "        numerical_col_name = list(aggregation.keys())[0]\n",
    "        agg_func = list(aggregation.values())[0]\n",
    "        new_col_name = create_column_name(agg_func, filters, numerical_col_name)\n",
    "        spark_auto_col_name = agg_func + \"(\" + numerical_col_name + \")\" # result_df.columns[-1]\n",
    "        result_df = result_df.withColumnRenamed(spark_auto_col_name, new_col_name)\n",
    "\n",
    "        list_of_dataframes.append(result_df)\n",
    "\n",
    "    all_combined_result_df = reduce(lambda df1, df2: df1.union(df2), list_of_dataframes)\n",
    "    return all_combined_result_df\n",
    "\n",
    "\n",
    "\n",
    "# Chiamata al metodo _selecting\n",
    "data_df.show()\n",
    "\n",
    "result_df = _selecting(data_df, [\"id\", \"bucket\"], [(\"categorical_feature_1\", [\"spazzolini\", \"spezie\"])], {\"numerical_1\": \"sum\"})\n",
    "result_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+-----------+---------------------+---------------------+------------------------------------------+\n",
      "|id |timestamp          |numerical_1|numerical_2|categorical_feature_1|categorical_feature_2|bucket                                    |\n",
      "+---+-------------------+-----------+-----------+---------------------+---------------------+------------------------------------------+\n",
      "|1  |2023-06-01 00:00:00|1          |2          |pasta                |bitcoin              |{2023-05-31 00:00:00, 2023-06-02 00:00:00}|\n",
      "|1  |2023-06-01 00:00:00|2          |3          |pasta                |cash                 |{2023-05-31 00:00:00, 2023-06-02 00:00:00}|\n",
      "|1  |2023-06-01 00:00:00|4          |1          |spezie               |bancomat             |{2023-05-31 00:00:00, 2023-06-02 00:00:00}|\n",
      "|1  |2023-06-01 00:00:00|6          |1          |spazzolini           |bancomat             |{2023-05-31 00:00:00, 2023-06-02 00:00:00}|\n",
      "|1  |2023-06-02 00:00:00|7          |6          |pasta                |bancomat             |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|\n",
      "|1  |2023-06-02 00:00:00|7          |6          |spezie               |bancomat             |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|\n",
      "|1  |2023-06-06 00:00:00|4          |2          |pasta                |cash                 |{2023-06-06 00:00:00, 2023-06-08 00:00:00}|\n",
      "|2  |2023-06-03 00:00:00|10         |12         |pasta                |cash                 |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|\n",
      "|2  |2023-06-03 00:00:00|13         |15         |spazzolini           |cash                 |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|\n",
      "|2  |2023-06-03 00:00:00|1          |15         |spazzolini           |cash                 |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|\n",
      "+---+-------------------+-----------+-----------+---------------------+---------------------+------------------------------------------+\n",
      "\n",
      "+---+------------------------------------------+----------------+---------------------------------------------------------------+\n",
      "|id |bucket                                    |avg(numerical_2)|sum_of_numerical_1_by_categorical_feature_1_(spazzolini_spezie)|\n",
      "+---+------------------------------------------+----------------+---------------------------------------------------------------+\n",
      "|1  |{2023-05-31 00:00:00, 2023-06-02 00:00:00}|1.0             |10                                                             |\n",
      "|1  |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|6.0             |7                                                              |\n",
      "|2  |{2023-06-02 00:00:00, 2023-06-04 00:00:00}|15.0            |14                                                             |\n",
      "+---+------------------------------------------+----------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, functions as F\n",
    "\n",
    "### VERSIONE FUNZIONANTE PIU EFFICIENTE\n",
    "\n",
    "\n",
    "def _selecting(df: DataFrame, identifier_cols_name: List[str], filters: List[Tuple[str, List[str]]], aggregation: Dict[str, str]) -> DataFrame:\n",
    "    # Applicare i filtri alle variabili categoriche\n",
    "    filter_conditions = []\n",
    "    for single_filter in filters:\n",
    "        filter_column, filter_options = single_filter\n",
    "        filter_conditions.append(F.col(filter_column).isin(filter_options))\n",
    "    \n",
    "    filter_expression = reduce(lambda f1, f2: f1 & f2, filter_conditions)\n",
    "    filtered_df = df.filter(filter_expression)\n",
    "\n",
    "    # Aggregare i dati per ottenere la somma su ogni finestra\n",
    "    result_df = filtered_df.groupBy(*identifier_cols_name).agg(aggregation)\n",
    "\n",
    "    # Rinominare le colonne risultanti usando la funzione di utilità\n",
    "    numerical_col_name = list(aggregation.keys())[0]\n",
    "    agg_func = list(aggregation.values())[0]\n",
    "    new_col_name = create_column_name(agg_func, filters, numerical_col_name)\n",
    "    result_df = result_df.withColumnRenamed(agg_func + \"(\" + numerical_col_name + \")\", new_col_name)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "data_df.show(truncate=False)\n",
    "result_df = _selecting(data_df, [\"id\", \"bucket\"], [(\"categorical_feature_1\", [\"spazzolini\", \"spezie\"])], {\"numerical_1\": \"sum\", \"numerical_2\": \"avg\"})\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join_dataframes(df1: DataFrame, df2: DataFrame, join_columns: list[str]) -> DataFrame:\n",
    "    return df1.join(df2, join_columns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo:\n",
    "# ciclo su tutte le possibili aggregazioni\n",
    "##### creo una colonna per ogni elemento della lista\n",
    "##### se l'elemento della lista contiene solo una tuple allora la mando a fare pivot\n",
    "######## il pivot lo fa sulla colonna messa come secondo elemento della tupla \n",
    "##### se come secondo elemento troviamo * allora non si mettono filtri nel pivot e lo fa su tutte\n",
    "####### se l'elemento della lista contiene più di una tupla allora la mando a fare la select\n",
    "####### tutte le opzioni di una stessa categorica vanno in or (prendi la somma di tutti i soldi spesi in cibo o in sigarette)\n",
    "####### tutti gli elementi di categorie diverse vanno in and (prendi la somma di tutti i soldi spesi in (cibo or sigarette) and (pagari in contanti)\n",
    "# faccio il join tra tutti i df che ho creato\n",
    "\n",
    "def _aggregation_with_filter(grouped_df: DataFrame, all_aggregation_filters: list[list[tuple[str,list[str]]]], numerical_col_name: list[str],agg_funcs: list[str], identifier_cols_name: list[str]) -> DataFrame:\n",
    "    extended_id_cols_name = identifier_cols_name + [\"bucket\"]\n",
    "    \n",
    "    all_aggregations = _all_aggregation_combination(numerical_col_name, agg_funcs)\n",
    "    #grouped_df = _grouping(data_df, extended_id_cols_name)\n",
    "\n",
    "    all_aggregated_df = []\n",
    "\n",
    "    for aggregation in all_aggregations:\n",
    "        for aggregation_filter in all_aggregation_filters:\n",
    "            if len(aggregation_filter) > 1:\n",
    "                print(\"in selectiong\")\n",
    "                filtered_df = _selecting(grouped_df, aggregation_filter, aggregation)\n",
    "                all_aggregated_df.append(filtered_df)\n",
    "            else: \n",
    "                print(\"in pivoting\")\n",
    "                pivoted_df = _pivoting(grouped_df, aggregation_filter, aggregation)\n",
    "                all_aggregated_df.append(pivoted_df)\n",
    "               \n",
    "    \n",
    "    df_final = reduce(lambda df1, df2: _join_dataframes(df1, df2, extended_id_cols_name), all_aggregated_df)\n",
    "\n",
    "    return df_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in pivoting\n",
      "pivot senza filtri\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'pivot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m aggregation_filters: \u001b[39mlist\u001b[39m[\u001b[39mlist\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]]]] \u001b[39m=\u001b[39m   [   \n\u001b[1;32m     18\u001b[0m                             [(\u001b[39m\"\u001b[39m\u001b[39mcategorical_feature_1\u001b[39m\u001b[39m\"\u001b[39m,[])],\n\u001b[1;32m     19\u001b[0m                         ]\n\u001b[1;32m     20\u001b[0m agg_funcs \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m]  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m aggregated_df \u001b[39m=\u001b[39m _aggregation_with_filter(data_df, aggregation_filters, numerical_col_name, agg_funcs, identifier_cols_name)\n\u001b[1;32m     24\u001b[0m aggregated_df\u001b[39m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36m_aggregation_with_filter\u001b[0;34m(grouped_df, all_aggregation_filters, numerical_col_name, agg_funcs, identifier_cols_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[39melse\u001b[39;00m: \n\u001b[1;32m     27\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39min pivoting\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m             pivoted_df \u001b[39m=\u001b[39m _pivoting(grouped_df, aggregation_filter, aggregation)\n\u001b[1;32m     29\u001b[0m             all_aggregated_df\u001b[39m.\u001b[39mappend(pivoted_df)\n\u001b[1;32m     32\u001b[0m df_final \u001b[39m=\u001b[39m reduce(\u001b[39mlambda\u001b[39;00m df1, df2: _join_dataframes(df1, df2, extended_id_cols_name), all_aggregated_df)\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36m_pivoting\u001b[0;34m(grouped_df, filter, aggregation)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cat_options) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpivot senza filtri\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     pivoted_df \u001b[39m=\u001b[39m grouped_df\u001b[39m.\u001b[39;49mpivot(cat_var_name)\u001b[39m.\u001b[39magg(aggregation)\n\u001b[1;32m      8\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[1;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpivot con filtri di opzioni\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ts_train/lib/python3.9/site-packages/pyspark/sql/dataframe.py:2977\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2944\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   2945\u001b[0m \n\u001b[1;32m   2946\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2974\u001b[0m \u001b[39m+---+\u001b[39;00m\n\u001b[1;32m   2975\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2976\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m-> 2977\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   2978\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name)\n\u001b[1;32m   2979\u001b[0m     )\n\u001b[1;32m   2980\u001b[0m jc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mapply(name)\n\u001b[1;32m   2981\u001b[0m \u001b[39mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pivot'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 10:27:12 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "#pipeline\n",
    "#configurazione di input:\n",
    "''' \n",
    "numerical_col_name = [\"numerical_1\", \"numerical_2\"]\n",
    "identifier_cols_name = [\"id\"]  # type: ignore\n",
    "aggregation_filters =   [   \n",
    "                            [ (\"categorical_feature_1\", [\"pasta\",\"spezie\"])],\n",
    "                            [ (\"categorical_feature_1\", [\"pasta\"]), (\"categorical_feature_1\",[\"spezie\"]) ],\n",
    "                            [(\"categorical_feature_1\",[\"pasta\",\"spezie\"]), (\"categorical_feature_2\",[\"cash\"]) ],\n",
    "                ]\n",
    "agg_funcs = [\"sum\", \"avg\"]  # type: ignore\n",
    "'''\n",
    "\n",
    "# bug noto: non mi rinomina le colonne che crea\n",
    "numerical_col_name = [\"numerical_1\", \"numerical_2\"]\n",
    "identifier_cols_name = [\"id\"]  # type: ignore\n",
    "aggregation_filters: list[list[tuple[str, list[str]]]] =   [   \n",
    "                            [(\"categorical_feature_1\",[])],\n",
    "                        ]\n",
    "agg_funcs = [\"sum\"]  # type: ignore\n",
    "\n",
    "\n",
    "aggregated_df = _aggregation_with_filter(data_df, aggregation_filters, numerical_col_name, agg_funcs, identifier_cols_name)\n",
    "aggregated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------+-----------+---------------------+---------------------+--------------------+\n",
      "| id|          timestamp|numerical_1|numerical_2|categorical_feature_1|categorical_feature_2|              window|\n",
      "+---+-------------------+-----------+-----------+---------------------+---------------------+--------------------+\n",
      "|  1|2023-06-01 00:00:00|          1|          2|                pasta|              bitcoin|{2023-05-31 00:00...|\n",
      "|  1|2023-06-01 00:00:00|          2|          3|                pasta|                 cash|{2023-05-31 00:00...|\n",
      "|  1|2023-06-01 00:00:00|          4|          1|               spezie|             bancomat|{2023-05-31 00:00...|\n",
      "|  1|2023-06-01 00:00:00|          6|          1|           spazzolini|             bancomat|{2023-05-31 00:00...|\n",
      "|  1|2023-06-02 00:00:00|          7|          6|                pasta|             bancomat|{2023-06-02 00:00...|\n",
      "|  1|2023-06-06 00:00:00|          4|          2|                pasta|                 cash|{2023-06-06 00:00...|\n",
      "+---+-------------------+-----------+-----------+---------------------+---------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 10:49:57 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
