{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from ts_train.step.time_bucketing import TimeBucketing  # type: ignore\n",
    "from ts_train.step.filling import Filling  # type: ignore\n",
    "from ts_train.step.aggregation import Aggregation  # type: ignore\n",
    "from pyspark_assert import assert_frame_equal\n",
    "from pyspark_assert._assertions import DifferentSchemaAssertionError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/24 11:48:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timestamps_struct(\n",
    "    df,\n",
    "    cols_name,\n",
    "    struct_col_name: str,\n",
    "    struct_fields_name = (\"start\", \"end\"),\n",
    "    format: str = \"yyyy-MM-dd\",\n",
    "):\n",
    "    return df.withColumn(\n",
    "        struct_col_name,\n",
    "        F.struct(\n",
    "            F.to_timestamp(F.col(cols_name[0]), format).alias(struct_fields_name[0]),\n",
    "            F.to_timestamp(F.col(cols_name[1]), format).alias(struct_fields_name[1]),\n",
    "        ),\n",
    "    ).drop(*cols_name)\n",
    "\n",
    "def sample_dataframe_02(spark):\n",
    "\n",
    "    \n",
    "    df = spark.createDataFrame(\n",
    "        [\n",
    "            (348272371, \"2023-01-01\", \"2023-01-02\", 61, 55, 97, 348272371),\n",
    "            (348272371, \"2023-01-06\", \"2023-01-07\", None, 1354, None, 348272371),\n",
    "            (234984832, \"2023-01-01\", \"2023-01-02\", 1298, None, None, 234984832),\n",
    "            (234984832, \"2023-01-02\", \"2023-01-03\", None, None, 22, 234984832),\n",
    "        ],\n",
    "        schema=[\n",
    "            \"ID_BIC_CLIENTE\",\n",
    "            \"bucket_start\",\n",
    "            \"bucket_end\",\n",
    "            \"salute\",\n",
    "            \"shopping\",\n",
    "            \"trasporti\",\n",
    "            \"ID_BIC_CLIENTE_2\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return create_timestamps_struct(\n",
    "        df=df, cols_name=(\"bucket_start\", \"bucket_end\"), struct_col_name=\"bucket\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLICATED VERSION\n",
    "\n",
    "from pyspark.sql.functions import col, lag \n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def test_process_samples_timestamp_distance_with_spark_utility(\n",
    "    spark, sample_dataframe_pre_filling\n",
    "):\n",
    "    time_bucket_size = 10 \n",
    "    time_bucket_granularity = \"days\"\n",
    "\n",
    "    time_column_name = \"timestamp\"\n",
    "    identifier_cols_name = [\"ID_BIC_CLIENTE\", \"ID_BIC_CLIENTE_2\"]\n",
    "\n",
    "    # Tests that the difference between each sample for each id is one day.\n",
    "    standard_filling = Filling(\n",
    "        time_bucket_col_name=\"bucket\",\n",
    "        identifier_cols_name=identifier_cols_name,\n",
    "        time_bucket_size=time_bucket_size,\n",
    "        time_bucket_granularity=time_bucket_granularity,\n",
    "    )\n",
    "\n",
    "    df_after_filling = standard_filling(df=sample_dataframe_pre_filling, spark=spark)\n",
    "\n",
    "    # Check if the specified column contains any null values\n",
    "    contains_nulls = df_after_filling.where(col(time_column_name).isNull()).count() > 0\n",
    "    assert not contains_nulls\n",
    "\n",
    "    # Create a Window specification with partitioning by 'ID_BIC_CLIENTE' and 'altro', and ordering by 'timestamp'\n",
    "    window_spec = Window.partitionBy(*identifier_cols_name).orderBy(time_column_name)\n",
    "\n",
    "    # Calculate the time differences between all timestamps\n",
    "    df_after_filling = df_after_filling.withColumn(\n",
    "        f\"shifted_{time_column_name}\", lag(time_column_name, 1).over(window_spec)\n",
    "    )\n",
    "    df_after_filling = df_after_filling.withColumn(\n",
    "        \"time_diff\",\n",
    "        col(time_column_name).cast(\"long\") - col(f\"shifted_{time_column_name}\").cast(\"long\"),\n",
    "    )\n",
    "\n",
    "    # count the number of unique combinations of identifier_cols_name\n",
    "    num_unique_combinations = df_after_filling.select(*identifier_cols_name).distinct().count()\n",
    "    # count the number of null values in the time_diff column\n",
    "    num_null_time_diff = df_after_filling.filter(col(\"time_diff\").isNull()).count()\n",
    "    assert num_unique_combinations == num_null_time_diff\n",
    "\n",
    "    if df_after_filling.count() > num_null_time_diff:\n",
    "        difference_between_timestamps = len(\n",
    "            df_after_filling.select(F.collect_set(\"time_diff\")).collect()[0][0]\n",
    "        )\n",
    "        assert difference_between_timestamps == 1\n",
    "\n",
    "    df_after_filling.show(truncate=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+---------+----------------+------------------------------------------+\n",
      "|ID_BIC_CLIENTE|salute|shopping|trasporti|ID_BIC_CLIENTE_2|bucket                                    |\n",
      "+--------------+------+--------+---------+----------------+------------------------------------------+\n",
      "|348272371     |61    |55      |97       |348272371       |{2023-01-01 00:00:00, 2023-01-02 00:00:00}|\n",
      "|348272371     |null  |1354    |null     |348272371       |{2023-01-06 00:00:00, 2023-01-07 00:00:00}|\n",
      "|234984832     |1298  |null    |null     |234984832       |{2023-01-01 00:00:00, 2023-01-02 00:00:00}|\n",
      "|234984832     |null  |null    |22       |234984832       |{2023-01-02 00:00:00, 2023-01-03 00:00:00}|\n",
      "+--------------+------+--------+---------+----------------+------------------------------------------+\n",
      "\n",
      "+--------------+----------------+-------------------+------+--------+---------+-----------------+---------+\n",
      "|ID_BIC_CLIENTE|ID_BIC_CLIENTE_2|timestamp          |salute|shopping|trasporti|shifted_timestamp|time_diff|\n",
      "+--------------+----------------+-------------------+------+--------+---------+-----------------+---------+\n",
      "|234984832     |234984832       |2023-01-01 00:00:00|1298  |0       |0        |null             |null     |\n",
      "|348272371     |348272371       |2023-01-01 00:00:00|61    |55      |97       |null             |null     |\n",
      "+--------------+----------------+-------------------+------+--------+---------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_dataframe = sample_dataframe_02(spark)\n",
    "sample_dataframe.show(truncate=False)\n",
    "test_process_samples_timestamp_distance_with_spark_utility(spark, sample_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SIMPLE VERSION \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, ArrayType\n",
    "\n",
    "def test_process_samples_timestamp_distance(\n",
    "    spark, sample_dataframe_02\n",
    "):\n",
    "    time_column_name = \"timestamp\"\n",
    "    identifier_cols_name = \"ID_BIC_CLIENTE\"\n",
    "\n",
    "    \"\"\"Tests that the difference between each sample for each id is one day.\"\"\"\n",
    "    time_bucket_size = 20\n",
    "    time_bucket_granularity = \"minutes\"\n",
    "    \n",
    "    standard_filling = Filling(\n",
    "        time_bucket_col_name=\"bucket\",\n",
    "        identifier_cols_name=identifier_cols_name,\n",
    "        time_bucket_size=time_bucket_size,\n",
    "        time_bucket_granularity=time_bucket_granularity,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Executes the time bucketing step and the filling step\n",
    "    df_after_filling = standard_filling(df=sample_dataframe_02, spark=spark)\n",
    "    df_after_filling.show(truncate=False)\n",
    "\n",
    "    df_after_filling.show()\n",
    "     \n",
    "    # Check if the specified column contains any null values\n",
    "    contains_nulls = df_after_filling.where(col(time_column_name).isNull()).count() > 0\n",
    "    assert not contains_nulls\n",
    "   \n",
    "\n",
    "    # Convert the 'timestamp' column to Unix timestamp\n",
    "    df_after_filling = df_after_filling.withColumn(\"timestamp_unix\", F.unix_timestamp(\"timestamp\"))\n",
    "\n",
    "    # Group by 'ID_BIC_CLIENTE' and collect the list of all 'timestamp_unix' values for each user\n",
    "    timestamps_per_user = df_after_filling.groupBy(\"ID_BIC_CLIENTE\").agg(F.collect_list(\"timestamp_unix\").alias(\"timestamps_list\"))\n",
    "\n",
    "\n",
    "\n",
    "    all_users = timestamps_per_user.select(\"ID_BIC_CLIENTE\").distinct().collect()\n",
    "    for user in all_users:\n",
    "        user_timestamps = timestamps_per_user.filter(timestamps_per_user[\"ID_BIC_CLIENTE\"] == user[0]).select(\"timestamps_list\").collect()[0][0]\n",
    "        # Calculate the differences between each element and the next one using list comprehension\n",
    "        differences = [user_timestamps[i+1] - user_timestamps[i] for i in range(len(user_timestamps) - 1)]\n",
    "\n",
    "        # Check if all differences are equal\n",
    "        assert all(difference == differences[0] for difference in differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
