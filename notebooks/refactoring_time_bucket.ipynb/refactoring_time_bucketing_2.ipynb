{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changelog\n",
    "\n",
    "* il timebucket non parte dal giorno prima ma dalla mezzanotte dal punto 0 del giorno corrente (aggiustato per la granularità)\n",
    "\n",
    "* il timebucket supporta nativamente h, d, w, m, y\n",
    "\n",
    "* il timebucket non è soggetto a problemi di timezone/summer time\n",
    "\n",
    "* maggior chiarezza del range dei timebucket (es. 1h: 00:00:00 - 00:59:59)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "* Riconvertire i bucket_start e bucket_end in datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from ts_train.step.core import AbstractPipelineStep\n",
    "from ts_train.step.time_bucketing import TimeBucketing\n",
    "from pyspark.sql import Row\n",
    "from pydantic import BaseModel, StrictStr\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Codice per visualizzazione su notebook\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import DateOffset\n",
    "    \n",
    "def get_data_offset(time_bucket_size,time_bucket_granularity) -> DateOffset:\n",
    "    \"\"\"\n",
    "    Get the offset for the provided bucket size and granularity.\n",
    "\n",
    "    Returns:\n",
    "        offset (DateOffset): Offset for the provided bucket size and granularity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Available Granularity:\n",
    "    # https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n",
    "\n",
    "    if time_bucket_granularity[0].upper() == \"H\":\n",
    "        return DateOffset(hours=time_bucket_size)\n",
    "    elif time_bucket_granularity[0].upper() == \"D\":\n",
    "        return DateOffset(days=time_bucket_size)\n",
    "    elif time_bucket_granularity[0].upper() == \"W\":\n",
    "        return DateOffset(weeks=time_bucket_size)\n",
    "    elif time_bucket_granularity[0].upper() == \"M\":\n",
    "        return DateOffset(months=time_bucket_size)\n",
    "    elif time_bucket_granularity[0].upper() == \"Y\":\n",
    "        return DateOffset(years=time_bucket_size)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Granularity {time_bucket_granularity} not supported\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, lit\n",
    "from typing import List\n",
    "\n",
    "def test_null_values(df, time_column_name: str) -> None:\n",
    "    # Check if the specified column contains any null values\n",
    "    contains_nulls = df.where(col(time_column_name).isNull()).count() > 0\n",
    "    assert not contains_nulls, f\"Column '{time_column_name}' contains null values.\"\n",
    "\n",
    "# Verifica che non ci sia stata corruzione di dai durante la conversione da pandas a spark\n",
    "def test_time_range(bucket_df, min_date, max_date):\n",
    "    min_date = str(min_date)\n",
    "    max_date = str(max_date)\n",
    "    first_element = bucket_df.first()[\"bucket_start\"]\n",
    "    last_element = bucket_df.tail(1)[0][\"bucket_end\"]\n",
    "\n",
    "    assert first_element == str(min_date), F\"first_element: {first_element} min_date: {min_date}\"\n",
    "\n",
    "    # since last_element also contains the offset, it should be bigger then the max_date \n",
    "    assert last_element > str(max_date), F\"last_element: {last_element} > max_date: {max_date}\"\n",
    "\n",
    "\n",
    "'''\n",
    "def test_bucket_size(date_range, time_bucket_size, time_bucket_granularity) -> None:\n",
    "    if len(date_range) > 1:\n",
    "        \n",
    "        differences = [date_range[i] - date_range[i-1] for i in range(1, len(date_range))]\n",
    "        num_of_differences = len(set(differences))\n",
    "\n",
    "        if time_bucket_granularity.upper() == \"Y\":\n",
    "            max_num_possible_diffences = 2 \n",
    "        elif time_bucket_granularity.upper() == \"M\":\n",
    "            max_num_possible_diffences = 3\n",
    "        else:\n",
    "            max_num_possible_diffences = 1\n",
    "        \n",
    "        # some years are 365 days long, some are 366 days long, So for years we can have 1 or 2 differences            \n",
    "        assert num_of_differences <= max_num_possible_diffences, f\"num_of_differences: {num_of_differences} > 2, differences: {differences}, date_range: {date_range}\"\n",
    "        \n",
    "        \n",
    "        if time_bucket_granularity.upper() == \"H\" or time_bucket_granularity == \"D\":\n",
    "            bucket_size = pd.Timedelta(f\"{time_bucket_size}{time_bucket_granularity}\")\n",
    "            for i in range(1, len(date_range)):\n",
    "                assert date_range[i] - date_range[i-1] == bucket_size , f\"date_range[i] - date_range[i-1]: {date_range[i] - date_range[i-1]} != bucket_size: {bucket_size}\"\n",
    "'''\n",
    "\n",
    "\n",
    "def test_buckets_monotonicity(df, time_column_name: str) -> None:\n",
    "    all_dates = df.select(time_column_name).collect()\n",
    "    \n",
    "    for id_date in range(1,len(all_dates)):\n",
    "        assert all_dates[id_date] > all_dates[id_date-1], f\"during test for {time_column_name}, at row {id_date}, the date {all_dates[id_date]} is <= then date at {id_date} that is {all_dates[id_date-1]}\"\n",
    "        \n",
    "\n",
    "    \n",
    "def test_all_buckets_are_equidistant_multi_user(df, time_column_name: str, identifier_cols_name: List[str]) -> None:\n",
    "    df = df.withColumn(\n",
    "        \"timestamp_unix\", F.unix_timestamp(time_column_name)\n",
    "    )\n",
    "    timestamps_per_user = df.groupBy(*identifier_cols_name).agg(\n",
    "        F.collect_list(\"timestamp_unix\").alias(\"timestamps_list\")\n",
    "    )\n",
    "    all_users = timestamps_per_user.select(*identifier_cols_name).distinct().collect()\n",
    "    for user_row in all_users:\n",
    "        user_identifier_values = user_row.asDict()\n",
    "        # Build a single filter condition for all identifier columns\n",
    "        filter_condition = (\n",
    "            col(col_name) == lit(col_value)\n",
    "            for col_name, col_value in user_identifier_values.items()\n",
    "        )\n",
    "        user_timestamps = (\n",
    "            timestamps_per_user.filter(reduce(lambda x, y: x & y, filter_condition))\n",
    "            .select(\"timestamps_list\")\n",
    "            .collect()[0][0]\n",
    "        )\n",
    "        # Calculate the differences between each element and the next one\n",
    "        # using list comprehension\n",
    "        differences = [\n",
    "            user_timestamps[i + 1] - user_timestamps[i]\n",
    "            for i in range(len(user_timestamps) - 1)\n",
    "        ]\n",
    "\n",
    "        # Check if all differences are equal\n",
    "        assert all(difference == differences[0] for difference in differences)\n",
    "\n",
    "def test_all_transactions_are_in_the_correct_bucket(df, time_column_name):\n",
    "    df = df.withColumn(\"data\", F.unix_timestamp(time_column_name))\n",
    "    df = df.withColumn(\"bucket_start\", F.unix_timestamp(\"bucket_start\"))\n",
    "    df = df.withColumn(\"bucket_end\", F.unix_timestamp(\"bucket_end\"))\n",
    "    \n",
    "    all_dates = [row for row in df.select(\"data\").collect()]\n",
    "    all_bucket_starts = [row for row in df.select(\"bucket_start\").collect()]\n",
    "    all_bucket_ends = [row for row in df.select(\"bucket_end\").collect()]\n",
    "\n",
    "    for i in range(len(all_dates)):\n",
    "        assert all_bucket_starts[i] <= all_dates[i] < all_bucket_ends[i], f\"{all_bucket_starts[i]} <= {all_dates[i]} < {all_bucket_ends[i]} is False for i = {i}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/22 15:30:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------+\n",
      "|id_utente|               data|importo|\n",
      "+---------+-------------------+-------+\n",
      "|        1|2023-03-25 01:00:00|   40.0|\n",
      "|        1|2023-03-25 02:00:00|   40.0|\n",
      "|        1|2023-03-25 04:00:00|   70.0|\n",
      "|        1|2023-03-28 04:00:00|   30.0|\n",
      "|        2|2023-03-25 04:00:00|   20.0|\n",
      "|        2|2023-03-25 22:00:00|   50.0|\n",
      "|        2|2023-03-26 03:00:00|   23.0|\n",
      "|        2|2023-03-26 05:00:00|   40.0|\n",
      "|        3|2023-03-21 02:00:00|   40.0|\n",
      "|        3|2023-03-23 07:00:00|   23.0|\n",
      "|        3|2023-03-25 23:00:00|   20.0|\n",
      "|        3|2023-03-25 23:00:00|   60.0|\n",
      "|        3|2023-03-26 03:00:00|   30.0|\n",
      "|        3|2023-03-28 02:00:00|   60.0|\n",
      "+---------+-------------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Date fornite\n",
    "data = [\n",
    "    (\"2023-03-25 23:00:00\",),\n",
    "    (\"2023-03-28 04:00:00\",),\n",
    "    (\"2023-03-26 05:00:00\",),\n",
    "    (\"2023-03-23 07:00:00\",),\n",
    "    (\"2023-03-25 01:00:00\",),\n",
    "    (\"2023-03-25 22:00:00\",),\n",
    "    (\"2023-03-25 23:00:00\",),\n",
    "    (\"2023-03-25 04:00:00\",),\n",
    "    (\"2023-03-25 04:00:00\",),\n",
    "    (\"2023-03-26 02:00:00\",),\n",
    "    (\"2023-03-25 02:00:00\",),\n",
    "    (\"2023-03-26 02:00:00\",),\n",
    "    (\"2023-03-21 02:00:00\",),\n",
    "    (\"2023-03-28 02:00:00\",),\n",
    "]\n",
    "\n",
    "# Converte le date fornite in oggetti datetime\n",
    "dates = [datetime.strptime(date[0], \"%Y-%m-%d %H:%M:%S\") for date in data]\n",
    "\n",
    "# Genera altri id_utente e importo in modo deterministico\n",
    "user_ids = [1, 2, 3]\n",
    "importo_values = [20.0, 30.0, 40.0, 23., 40.0,  50.0, 60.0, 70.0, 20.0, 30.0, 40.0, 23., 40.0,  50.0, 60.0]  # Valori deterministici\n",
    "\n",
    "# Genera le righe del DataFrame\n",
    "data_rows = []\n",
    "for i in range(len(dates)-1):\n",
    "    data_rows.append((user_ids[i % len(user_ids)-1], dates[i], importo_values[i]))\n",
    "\n",
    "data_rows.append((3, dates[-1], importo_values[-1]))\n",
    "\n",
    "# Crea il DataFrame\n",
    "df = pd.DataFrame(data_rows, columns=[\"id_utente\", \"data\", \"importo\"])\n",
    "\n",
    "sorted_df = df.sort_values(by=['id_utente', 'data'])\n",
    "\n",
    "# Crea la sessione Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Date Addition with date_add\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "original_df = spark.createDataFrame(sorted_df)\n",
    "\n",
    "# Stampa il DataFrame\n",
    "original_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizzo time bucket step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------+-------------------+-------------------+\n",
      "|id_utente|data               |importo|bucket_start       |bucket_end         |\n",
      "+---------+-------------------+-------+-------------------+-------------------+\n",
      "|1        |2023-03-25 01:00:00|40.0   |2023-03-25 00:00:00|2023-03-25 23:59:59|\n",
      "|1        |2023-03-25 02:00:00|40.0   |2023-03-25 00:00:00|2023-03-25 23:59:59|\n",
      "|1        |2023-03-25 04:00:00|70.0   |2023-03-25 00:00:00|2023-03-25 23:59:59|\n",
      "|1        |2023-03-28 04:00:00|30.0   |2023-03-28 00:00:00|2023-03-28 23:59:59|\n",
      "|2        |2023-03-25 04:00:00|20.0   |2023-03-25 00:00:00|2023-03-25 23:59:59|\n",
      "|2        |2023-03-25 22:00:00|50.0   |2023-03-25 00:00:00|2023-03-25 23:59:59|\n",
      "|2        |2023-03-26 03:00:00|23.0   |2023-03-26 00:00:00|2023-03-26 23:59:59|\n",
      "|2        |2023-03-26 05:00:00|40.0   |2023-03-26 00:00:00|2023-03-26 23:59:59|\n",
      "|3        |2023-03-21 02:00:00|40.0   |2023-03-21 00:00:00|2023-03-21 23:59:59|\n",
      "|3        |2023-03-23 07:00:00|23.0   |2023-03-23 00:00:00|2023-03-23 23:59:59|\n",
      "|3        |2023-03-25 23:00:00|20.0   |2023-03-25 00:00:00|2023-03-25 23:59:59|\n",
      "|3        |2023-03-25 23:00:00|60.0   |2023-03-25 00:00:00|2023-03-25 23:59:59|\n",
      "|3        |2023-03-26 03:00:00|30.0   |2023-03-26 00:00:00|2023-03-26 23:59:59|\n",
      "|3        |2023-03-28 02:00:00|60.0   |2023-03-28 00:00:00|2023-03-28 23:59:59|\n",
      "+---------+-------------------+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_bucket_step = TimeBucketing(\n",
    "    time_column_name = \"data\",\n",
    "    time_bucket_size = 1,\n",
    "    time_bucket_granularity = \"days\",\n",
    ")\n",
    "\n",
    "time_bucket_df = time_bucket_step._process(original_df, spark)\n",
    "time_bucket_df = time_bucket_df.sort([\"id_utente\",\"data\"])\n",
    "time_bucket_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+-------+\n",
      "|id_utente|bucket_start       |bucket_end         |importo|\n",
      "+---------+-------------------+-------------------+-------+\n",
      "|1        |2023-03-25 00:00:00|2023-03-25 23:59:59|150.0  |\n",
      "|1        |2023-03-28 00:00:00|2023-03-28 23:59:59|30.0   |\n",
      "|2        |2023-03-25 00:00:00|2023-03-25 23:59:59|70.0   |\n",
      "|2        |2023-03-26 00:00:00|2023-03-26 23:59:59|63.0   |\n",
      "|3        |2023-03-21 00:00:00|2023-03-21 23:59:59|40.0   |\n",
      "|3        |2023-03-23 00:00:00|2023-03-23 23:59:59|23.0   |\n",
      "|3        |2023-03-25 00:00:00|2023-03-25 23:59:59|80.0   |\n",
      "|3        |2023-03-26 00:00:00|2023-03-26 23:59:59|30.0   |\n",
      "|3        |2023-03-28 00:00:00|2023-03-28 23:59:59|60.0   |\n",
      "+---------+-------------------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aggregated_df = time_bucket_df.groupBy([\"id_utente\", \"bucket_start\",\"bucket_end\"]).agg(F.sum(\"importo\").alias(\"importo\"))\n",
    "aggregated_df = aggregated_df.orderBy([\"id_utente\", \"bucket_start\"])\n",
    "\n",
    "#aggregated_df = aggregated_df.filter(aggregated_df[\"id_utente\"] == 2)\n",
    "\n",
    "aggregated_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione del filling step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+-------+-----------+---------+\n",
      "|id_utente|bucket_start       |bucket_end         |importo|id_utente_2|importo_2|\n",
      "+---------+-------------------+-------------------+-------+-----------+---------+\n",
      "|1        |2023-03-25 00:00:00|2023-03-25 23:59:59|150.0  |2          |2        |\n",
      "|1        |2023-03-28 00:00:00|2023-03-28 23:59:59|30.0   |2          |2        |\n",
      "|2        |2023-03-25 00:00:00|2023-03-25 23:59:59|70.0   |3          |3        |\n",
      "|2        |2023-03-26 00:00:00|2023-03-26 23:59:59|63.0   |3          |3        |\n",
      "|3        |2023-03-21 00:00:00|2023-03-21 23:59:59|40.0   |4          |4        |\n",
      "|3        |2023-03-23 00:00:00|2023-03-23 23:59:59|23.0   |4          |4        |\n",
      "|3        |2023-03-25 00:00:00|2023-03-25 23:59:59|80.0   |4          |4        |\n",
      "|3        |2023-03-26 00:00:00|2023-03-26 23:59:59|30.0   |4          |4        |\n",
      "|3        |2023-03-28 00:00:00|2023-03-28 23:59:59|60.0   |4          |4        |\n",
      "+---------+-------------------+-------------------+-------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_aggregated_df = aggregated_df\n",
    "# add new column fillled by random integers\n",
    "new_aggregated_df = new_aggregated_df.withColumn(\"id_utente_2\", col(\"id_utente\")+1)\n",
    "new_aggregated_df = new_aggregated_df.withColumn(\"importo_2\", col(\"id_utente\")+1)\n",
    "\n",
    "new_aggregated_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+-------------------+-------+---------+\n",
      "|id_utente|id_utente_2|bucket_start       |bucket_end         |importo|importo_2|\n",
      "+---------+-----------+-------------------+-------------------+-------+---------+\n",
      "|1        |2          |2023-03-25 00:00:00|2023-03-25 23:59:59|150.0  |2        |\n",
      "|1        |2          |2023-03-26 00:00:00|2023-03-26 23:59:59|0.0    |0        |\n",
      "|1        |2          |2023-03-27 00:00:00|2023-03-27 23:59:59|0.0    |0        |\n",
      "|1        |2          |2023-03-28 00:00:00|2023-03-28 23:59:59|30.0   |2        |\n",
      "|2        |3          |2023-03-25 00:00:00|2023-03-25 23:59:59|70.0   |3        |\n",
      "|2        |3          |2023-03-26 00:00:00|2023-03-26 23:59:59|63.0   |3        |\n",
      "|3        |4          |2023-03-21 00:00:00|2023-03-21 23:59:59|40.0   |4        |\n",
      "|3        |4          |2023-03-22 00:00:00|2023-03-22 23:59:59|0.0    |0        |\n",
      "|3        |4          |2023-03-23 00:00:00|2023-03-23 23:59:59|23.0   |4        |\n",
      "|3        |4          |2023-03-24 00:00:00|2023-03-24 23:59:59|0.0    |0        |\n",
      "|3        |4          |2023-03-25 00:00:00|2023-03-25 23:59:59|80.0   |4        |\n",
      "|3        |4          |2023-03-26 00:00:00|2023-03-26 23:59:59|30.0   |4        |\n",
      "|3        |4          |2023-03-27 00:00:00|2023-03-27 23:59:59|0.0    |0        |\n",
      "|3        |4          |2023-03-28 00:00:00|2023-03-28 23:59:59|60.0   |4        |\n",
      "+---------+-----------+-------------------+-------------------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def filling(df,new_time_bucket_step,identifier_cols_name):\n",
    "    # Creo la nuova timeline per tutti in pandas\n",
    "    new_time_bucket_step.time_column_name = \"bucket_start\"\n",
    "    \n",
    "    # Creates a list of identifier columns\n",
    "    identifier_cols = [\n",
    "        F.col(identifier_col_name)\n",
    "        for identifier_col_name in identifier_cols_name\n",
    "    ]\n",
    "\n",
    "    # Creates aliases for simplicity and code readability\n",
    "    time_bucket_start = f\"{time_bucket_col_name}_start\"\n",
    "    time_bucket_end = f\"{time_bucket_col_name}_end\"\n",
    "    min_time_bucket_start = f\"min_{time_bucket_col_name}_start\"\n",
    "    max_time_bucket_end = f\"max_{time_bucket_col_name}_end\"\n",
    "\n",
    "    # Creates a new DataFrame with only the identifier columns\n",
    "    # Splits the bucket into two column, start and end assigning to new columns\n",
    "    ids_df = df.select(\n",
    "        *identifier_cols,\n",
    "        F.col(\"bucket_start\").alias(time_bucket_start),\n",
    "        F.col(\"bucket_end\").alias(time_bucket_end),\n",
    "    )\n",
    "\n",
    "\n",
    "    # Takes only one record for every user\n",
    "    # Saves only the min start and the max end\n",
    "    ids_df = ids_df.groupBy(*identifier_cols).agg(\n",
    "        F.min(time_bucket_start).alias(min_time_bucket_start),\n",
    "        F.max(time_bucket_end).alias(max_time_bucket_end),\n",
    "    )\n",
    "    \n",
    "\n",
    "    # create the new timeline with every buckets \n",
    "    timeline,_,_ = time_bucket_step._create_timeline(df)\n",
    "    bucket_df = time_bucket_step._create_df_with_buckets(spark, timeline)\n",
    "    bucket_df = bucket_df.withColumn(\n",
    "        \"bucket_end\", expr(\"bucket_end - interval 1 second\")\n",
    "    )\n",
    "\n",
    "    # Collego gli utenti alla nuova timeline\n",
    "    # Converte le colonne delle date in tipo timestamp\n",
    "    bucket_df = bucket_df.withColumn(\"bucket_start\", col(\"bucket_start\").cast(\"timestamp\"))\n",
    "    bucket_df = bucket_df.withColumn(\"bucket_end\", col(\"bucket_end\").cast(\"timestamp\"))\n",
    "\n",
    "    # Esegue la join basata sulla condizione di intervallo\n",
    "    result_df = ids_df.join(\n",
    "        bucket_df,\n",
    "        (bucket_df['bucket_start'] >= ids_df[min_time_bucket_start]) &\n",
    "        (bucket_df['bucket_end'] <= ids_df[max_time_bucket_end])\n",
    "    )\n",
    "\n",
    "    # Seleziona le colonne desiderate per la tabella finale\n",
    "    all_timestamp_per_clients = result_df.select(*identifier_cols_name, \"bucket_start\", \"bucket_end\")\n",
    "    all_timestamp_per_clients = all_timestamp_per_clients.orderBy(identifier_cols_name+[\"bucket_start\"])\n",
    "    \n",
    "    # Joins the DataFrame with the new DataFrame in which has been generated\n",
    "    # timestamps for every user from its min timestamp to his max\n",
    "    # Fills with 0 null values of every column\n",
    "    # Drops time bucket column\n",
    "    join_on_cols = identifier_cols_name+[\"bucket_start\", \"bucket_end\"]\n",
    "    df = (\n",
    "        df.join(all_timestamp_per_clients, on=join_on_cols, how=\"right\")\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    df = df.orderBy(*join_on_cols)\n",
    "    return df\n",
    "\n",
    "identifier_cols_name = [\"id_utente\",\"id_utente_2\"]\n",
    "\n",
    "filled_df = filling(new_aggregated_df, time_bucket_step , identifier_cols_name)\n",
    "filled_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id_utente', LongType(), True), StructField('id_utente_2', LongType(), True), StructField('bucket_start', TimestampType(), True), StructField('bucket_end', TimestampType(), True), StructField('importo', DoubleType(), False), StructField('importo_2', LongType(), True)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filled_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+-------------------+-------+---------+\n",
      "|id_utente|id_utente_2|bucket_start       |bucket_end         |importo|importo_2|\n",
      "+---------+-----------+-------------------+-------------------+-------+---------+\n",
      "|1        |2          |2023-03-25 00:00:00|2023-03-25 23:59:59|150.0  |2        |\n",
      "|1        |2          |2023-03-26 00:00:00|2023-03-26 23:59:59|0.0    |0        |\n",
      "|1        |2          |2023-03-27 00:00:00|2023-03-27 23:59:59|0.0    |0        |\n",
      "|1        |2          |2023-03-28 00:00:00|2023-03-28 23:59:59|30.0   |2        |\n",
      "|2        |3          |2023-03-25 00:00:00|2023-03-25 23:59:59|70.0   |3        |\n",
      "|2        |3          |2023-03-26 00:00:00|2023-03-26 23:59:59|63.0   |3        |\n",
      "|3        |4          |2023-03-21 00:00:00|2023-03-21 23:59:59|40.0   |4        |\n",
      "|3        |4          |2023-03-22 00:00:00|2023-03-22 23:59:59|0.0    |0        |\n",
      "|3        |4          |2023-03-23 00:00:00|2023-03-23 23:59:59|23.0   |4        |\n",
      "|3        |4          |2023-03-24 00:00:00|2023-03-24 23:59:59|0.0    |0        |\n",
      "|3        |4          |2023-03-25 00:00:00|2023-03-25 23:59:59|80.0   |4        |\n",
      "|3        |4          |2023-03-26 00:00:00|2023-03-26 23:59:59|30.0   |4        |\n",
      "|3        |4          |2023-03-27 00:00:00|2023-03-27 23:59:59|0.0    |0        |\n",
      "|3        |4          |2023-03-28 00:00:00|2023-03-28 23:59:59|60.0   |4        |\n",
      "+---------+-----------+-------------------+-------------------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id_utente', LongType(), True), StructField('id_utente_2', LongType(), True), StructField('bucket_start', StringType(), True), StructField('bucket_end', TimestampType(), True), StructField('importo', DoubleType(), False), StructField('importo_2', LongType(), True)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format\n",
    "\n",
    "result_df = filled_df.withColumn(\"bucket_start\", date_format(col(\"bucket_start\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "result_df.show(truncate=False)\n",
    "result_df.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------+-------------------+-------+---------+\n",
      "|id_utente|id_utente_2|bucket_start       |bucket_end         |importo|importo_2|\n",
      "+---------+-----------+-------------------+-------------------+-------+---------+\n",
      "|1        |2          |2023-03-25 00:00:00|2023-03-25 23:59:59|150.0  |2        |\n",
      "|1        |2          |2023-03-26 00:00:00|2023-03-26 23:59:59|0.0    |0        |\n",
      "|1        |2          |2023-03-27 00:00:00|2023-03-27 23:59:59|0.0    |0        |\n",
      "|1        |2          |2023-03-28 00:00:00|2023-03-28 23:59:59|30.0   |2        |\n",
      "|2        |3          |2023-03-25 00:00:00|2023-03-25 23:59:59|70.0   |3        |\n",
      "|2        |3          |2023-03-26 00:00:00|2023-03-26 23:59:59|63.0   |3        |\n",
      "|3        |4          |2023-03-21 00:00:00|2023-03-21 23:59:59|40.0   |4        |\n",
      "|3        |4          |2023-03-22 00:00:00|2023-03-22 23:59:59|0.0    |0        |\n",
      "|3        |4          |2023-03-23 00:00:00|2023-03-23 23:59:59|23.0   |4        |\n",
      "|3        |4          |2023-03-24 00:00:00|2023-03-24 23:59:59|0.0    |0        |\n",
      "|3        |4          |2023-03-25 00:00:00|2023-03-25 23:59:59|80.0   |4        |\n",
      "|3        |4          |2023-03-26 00:00:00|2023-03-26 23:59:59|30.0   |4        |\n",
      "|3        |4          |2023-03-27 00:00:00|2023-03-27 23:59:59|0.0    |0        |\n",
      "|3        |4          |2023-03-28 00:00:00|2023-03-28 23:59:59|60.0   |4        |\n",
      "+---------+-----------+-------------------+-------------------+-------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id_utente', LongType(), True), StructField('id_utente_2', LongType(), True), StructField('bucket_start', StringType(), True), StructField('bucket_end', TimestampType(), True), StructField('importo', DoubleType(), False), StructField('importo_2', LongType(), True)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = filled_df.withColumn(\"bucket_start\", date_format(col(\"bucket_start\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "result_df.show(truncate=False)\n",
    "result_df.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
