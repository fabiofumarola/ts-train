{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    BooleanType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/20 09:24:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (348272371, \"2023-01-01\"),\n",
    "        (348272371, \"2023-01-02\"),\n",
    "        (348272371, \"2023-01-03\"),\n",
    "        (348272371, \"2023-01-04\"),\n",
    "        (348272371, \"2023-01-05\"),\n",
    "        (348272371, \"2023-01-06\"),\n",
    "        (348272371, \"2023-01-07\"),\n",
    "    ],\n",
    "    schema=[\n",
    "        \"ID_BIC_CLIENTE\",\n",
    "        \"DATA_TRANSAZIONE\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"DATA_TRANSAZIONE\", F.to_timestamp(F.col(\"DATA_TRANSAZIONE\"), \"yyyy-MM-dd\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|ID_BIC_CLIENTE|   DATA_TRANSAZIONE|\n",
      "+--------------+-------------------+\n",
      "|     348272371|2023-01-01 00:00:00|\n",
      "|     348272371|2023-01-02 00:00:00|\n",
      "|     348272371|2023-01-03 00:00:00|\n",
      "|     348272371|2023-01-04 00:00:00|\n",
      "|     348272371|2023-01-05 00:00:00|\n",
      "|     348272371|2023-01-06 00:00:00|\n",
      "|     348272371|2023-01-07 00:00:00|\n",
      "+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "for row in df.select(\"DATA_TRANSAZIONE\").collect():\n",
    "    print(type(row.DATA_TRANSAZIONE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2023, 1, 1, 0, 0),\n",
       " datetime.datetime(2023, 1, 2, 0, 0),\n",
       " datetime.datetime(2023, 1, 3, 0, 0),\n",
       " datetime.datetime(2023, 1, 4, 0, 0),\n",
       " datetime.datetime(2023, 1, 5, 0, 0),\n",
       " datetime.datetime(2023, 1, 6, 0, 0),\n",
       " datetime.datetime(2023, 1, 7, 0, 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_rows = df.select(\"DATA_TRANSAZIONE\").collect()\n",
    "datetimes = [datetime_row.DATA_TRANSAZIONE for datetime_row in datetime_rows]\n",
    "datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.datetime(2023, 1, 1, 0, 0), datetime.datetime(2023, 1, 2, 0, 0)),\n",
       " (datetime.datetime(2023, 1, 2, 0, 0), datetime.datetime(2023, 1, 3, 0, 0)),\n",
       " (datetime.datetime(2023, 1, 3, 0, 0), datetime.datetime(2023, 1, 4, 0, 0)),\n",
       " (datetime.datetime(2023, 1, 4, 0, 0), datetime.datetime(2023, 1, 5, 0, 0)),\n",
       " (datetime.datetime(2023, 1, 5, 0, 0), datetime.datetime(2023, 1, 6, 0, 0)),\n",
       " (datetime.datetime(2023, 1, 6, 0, 0), datetime.datetime(2023, 1, 7, 0, 0))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime_couples = [(datetime_1, datetime_2) for datetime_1, datetime_2 in zip(datetimes[::], datetimes[1::])]\n",
    "datetime_couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 day, 0:00:00\n",
      "1 day, 0:00:00\n",
      "1 day, 0:00:00\n",
      "1 day, 0:00:00\n",
      "1 day, 0:00:00\n",
      "1 day, 0:00:00\n"
     ]
    }
   ],
   "source": [
    "for datetime_couple in datetime_couples:\n",
    "    print(datetime_couple[1] - datetime_couple[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID_BIC_CLIENTE: bigint, DATA_TRANSAZIONE: date]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/20 09:24:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "def engine(pdf):\n",
    "    print(pdf.len())\n",
    "\n",
    "df.groupby(\"ID_BIC_CLIENTE\").applyInPandas(engine, schema=\"ID_BIC_CLIENTE long, DATA_TRANSAZIONE date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+---------+\n",
      "| id|               date|      unix|date_diff|\n",
      "+---+-------------------+----------+---------+\n",
      "|  1|2023-07-20 12:00:00|1689847200|     null|\n",
      "|  1|2023-07-21 12:00:00|1689933600|    86400|\n",
      "|  1|2023-07-22 12:00:00|1690020000|    86400|\n",
      "|  2|2023-07-21 12:00:00|1689933600|     null|\n",
      "|  2|2023-07-22 12:00:00|1690020000|    86400|\n",
      "+---+-------------------+----------+---------+\n",
      "\n",
      "Tutti gli intervalli di date sono validi (24 ore di differenza).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Creazione della SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CheckDateIntervals\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Creazione del DataFrame con dati casuali\n",
    "data = [\n",
    "    (1, \"2023-07-20 12:00:00\"),\n",
    "    (1, \"2023-07-22 12:00:00\"),\n",
    "    (1, \"2023-07-21 12:00:00\"),\n",
    "    (2, \"2023-07-22 12:00:00\"),\n",
    "    (2, \"2023-07-21 12:00:00\"),\n",
    "    # Aggiungi altre righe per testare diversi scenari\n",
    "]\n",
    "\n",
    "schema = [\"id\", \"date\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"date\", F.col(\"date\").cast(\"timestamp\"))\n",
    "\n",
    "df = df.withColumn(\"timestamp_unix\", F.unix_timestamp('date'))\n",
    "\n",
    "# Calcolo della differenza tra le date per ogni \"id\"\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(\"date\")\n",
    "\n",
    "df = df.withColumn(\"date_diff\", F.col(\"timestamp_unix\") - F.lag(F.col(\"timestamp_unix\"), 1).over(window_spec))\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Verifica della distanza di 24 ore tra le date\n",
    "invalid_intervals = df.filter((F.col(\"date_diff\") != 24*60*60) & F.col(\"date_diff\").isNotNull())\n",
    "\n",
    "# Stampa dei risultati\n",
    "if invalid_intervals.count() > 0:\n",
    "    print(\"Ci sono intervalli non validi per le date.\")\n",
    "    invalid_intervals.show()\n",
    "else:\n",
    "    print(\"Tutti gli intervalli di date sono validi (24 ore di differenza).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------+---------+\n",
      "| id|               date|timestamp_unix|date_diff|\n",
      "+---+-------------------+--------------+---------+\n",
      "|  1|2023-03-25 00:00:00|    1679698800|     null|\n",
      "|  1|2023-03-26 00:00:00|    1679785200|    86400|\n",
      "|  1|2023-03-27 00:00:00|    1679868000|    82800|\n",
      "|  2|2023-03-26 00:00:00|    1679785200|     null|\n",
      "|  2|2023-03-27 00:00:00|    1679868000|    82800|\n",
      "+---+-------------------+--------------+---------+\n",
      "\n",
      "Ci sono intervalli non validi per le date.\n",
      "+---+-------------------+--------------+---------+\n",
      "| id|               date|timestamp_unix|date_diff|\n",
      "+---+-------------------+--------------+---------+\n",
      "|  1|2023-03-27 00:00:00|    1679868000|    82800|\n",
      "|  2|2023-03-27 00:00:00|    1679868000|    82800|\n",
      "+---+-------------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Creazione della SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CheckDateIntervals\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Creazione del DataFrame con dati casuali\n",
    "data = [\n",
    "    (1, \"2023-03-25\"),\n",
    "    (1, \"2023-03-26\"),\n",
    "    (1, \"2023-03-27\"),\n",
    "    (2, \"2023-03-26\"),\n",
    "    (2, \"2023-03-27\"),\n",
    "    # Aggiungi altre righe per testare diversi scenari\n",
    "]\n",
    "\n",
    "schema = [\"id\", \"date\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"date\", F.col(\"date\").cast(\"timestamp\"))\n",
    "\n",
    "df = df.withColumn(\"timestamp_unix\", F.unix_timestamp('date'))\n",
    "\n",
    "# Calcolo della differenza tra le date per ogni \"id\"\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(\"date\")\n",
    "\n",
    "df = df.withColumn(\"date_diff\", F.col(\"timestamp_unix\") - F.lag(F.col(\"timestamp_unix\"), 1).over(window_spec))\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Verifica della distanza di 24 ore tra le date\n",
    "invalid_intervals = df.filter((F.col(\"date_diff\") != 24*60*60) & F.col(\"date_diff\").isNotNull())\n",
    "\n",
    "# Stampa dei risultati\n",
    "if invalid_intervals.count() > 0:\n",
    "    print(\"Ci sono intervalli non validi per le date.\")\n",
    "    invalid_intervals.show()\n",
    "else:\n",
    "    print(\"Tutti gli intervalli di date sono validi (24 ore di differenza).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
