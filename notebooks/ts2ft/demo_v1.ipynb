{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/07 21:12:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/07 21:12:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 21:13:05 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "#TODO: se nel df iniziale ci sono le colonne target, assicurarsi di non perderle dopo il bucketing. Creato il filtro per farlo ma serve il fix sui filters vuoti\n",
    "from pyspark.sql import SparkSession\n",
    "from ts_train.tr2ts.time_bucketing import TimeBucketing\n",
    "from ts_train.tr2ts.aggregating import Aggregating, Aggregation, Filter, AndGroup\n",
    "from ts_train.tr2ts.filling import Filling\n",
    "import numpy as np \n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Codice per visualizzazione su notebook\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "path_to_data = \"../../../dataset_offline/tr2ft/demo_v1/\"\n",
    "DATA_COLUMN_NAME = \"DATA_TRANSAZIONE\"\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"feature_generation\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Positive transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39682110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load positive dataset \n",
    "# 100k user for 39.682.111 40 million transactions\n",
    "positive_df_path = path_to_data + \"positive_target/100k_user_transactions.parquet\"\n",
    "positive_tr_df = spark.read.parquet(positive_df_path)\n",
    "positive_tr_df = positive_tr_df.withColumn(\"TARGET\", F.lit(1))\n",
    "\n",
    "print(positive_tr_df.count())\n",
    "print(positive_tr_df.select(\"ID_CLIENTE_BIC\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Negative transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 11:09:24 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# 100k user for 960.404 1 million transactions\n",
    "negative_df_path = path_to_data + \"negative_target/sample_transactions.parquet\"\n",
    "negative_tr_df = spark.read.parquet(negative_df_path)\n",
    "negative_tr_df = negative_tr_df.withColumn(\"TARGET\", F.lit(0))\n",
    "print(negative_tr_df.count())\n",
    "print(negative_tr_df.select(\"ID_CLIENTE_BIC\").distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create full transaction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 11:09:49 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/09/07 11:10:02 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/09/07 11:10:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/09/07 11:10:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/09/07 11:10:03 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join positive and negative dataset\n",
    "tr_df = positive_tr_df.union(negative_tr_df)\n",
    "# targets\n",
    "targets_df = tr_df.select(\"ID_CLIENTE_BIC\",\"TARGET\").distinct()\n",
    "tr_df = tr_df.drop(\"TARGET\")\n",
    "\n",
    "\n",
    "tr_df.write.parquet(path_to_data + \"transaction_dataset_df.parquet\")\n",
    "targets_df.write.parquet(path_to_data + \"targets_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = spark.read.parquet(path_to_data + \"transaction_dataset_df.parquet\")\n",
    "targets_df = spark.read.parquet(path_to_data + \"targets_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40642514"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dataset\n",
    "2000 utenti per un totale di 1 milione di transazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  40642514\n",
      "filtered:  1369803\n"
     ]
    }
   ],
   "source": [
    "TOTAL_ID_TO_MAINTAIN = 1000\n",
    "positive_id_to_mantain = targets_df.filter(targets_df.TARGET == 1).distinct().limit(TOTAL_ID_TO_MAINTAIN)\n",
    "negative_id_to_mantain = targets_df.filter(targets_df.TARGET == 0).distinct().limit(TOTAL_ID_TO_MAINTAIN)\n",
    "\n",
    "filtered_ids = positive_id_to_mantain.union(negative_id_to_mantain)\n",
    "\n",
    "# filtet tr_df with positive_id_to_mantain\n",
    "filterd_df = tr_df.join(filtered_ids, on=\"ID_CLIENTE_BIC\", how=\"inner\")\n",
    "print(\"filtered: \", filterd_df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transactions to timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 479:======================================================>(71 + 1) / 72]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+--------------+----------------+-------------------+-------+-----+--------+-----------+---------------+------+------+-------------------+-----------------------+-------------+-----+------+-----------+------+-------------------+-------------------+\n",
      "|ID_CLIENTE_BIC|ARCA_TIPO_CARTA|DATA_CONTABILE|DATA_TRANSAZIONE|ORA_TRANSAZIONE    |IMPORTO|SEGNO|IS_CARTA|TIPO_CANALE|TIPO_CANALE_AGG|IS_BON|IS_SDD|CATEGORY_LIV0      |CATEGORY_LIV1          |CATEGORY_LIV2|IS_CC|IS_LIB|MERCHANT   |TARGET|bucket_start       |bucket_end         |\n",
      "+--------------+---------------+--------------+----------------+-------------------+-------+-----+--------+-----------+---------------+------+------+-------------------+-----------------------+-------------+-----+------+-----------+------+-------------------+-------------------+\n",
      "|86772451      |EVOLUTION      |2021-07-08    |2020-12-17      |2023-08-28 14:36:26|3.5    |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2020-12-17 00:00:00|2020-12-17 23:59:59|\n",
      "|1196632408    |EVOLUTION      |2021-05-11    |2021-02-12      |2023-08-28 22:13:19|2.53   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-02-12 00:00:00|2021-02-12 23:59:59|\n",
      "|1188270032    |EVOLUTION      |2021-05-25    |2021-02-13      |2023-08-28 09:50:51|3.65   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-02-13 00:00:00|2021-02-13 23:59:59|\n",
      "|86772451      |EVOLUTION      |2021-05-07    |2021-02-18      |2023-08-28 18:37:23|3.07   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-02-18 00:00:00|2021-02-18 23:59:59|\n",
      "|86772451      |EVOLUTION      |2021-05-07    |2021-02-18      |2023-08-28 18:37:23|2.26   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-02-18 00:00:00|2021-02-18 23:59:59|\n",
      "|42693545      |CARTA_PREPAGATA|2021-06-11    |2021-02-27      |2023-08-28 00:00:00|22.3   |+    |true    |ALTRO      |ALTRO          |false |false |alimentari_spesa   |alimentari_supermercato|null         |false|false |null       |0     |2021-02-27 00:00:00|2021-02-27 23:59:59|\n",
      "|1188270032    |EVOLUTION      |2021-05-26    |2021-03-02      |2023-08-28 07:09:21|2.54   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-02 00:00:00|2021-03-02 23:59:59|\n",
      "|65123901      |EVOLUTION      |2021-06-04    |2021-03-04      |2023-08-28 05:50:55|2.81   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-04 00:00:00|2021-03-04 23:59:59|\n",
      "|1188270032    |EVOLUTION      |2021-06-01    |2021-03-08      |2023-08-28 13:46:38|1.39   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-08 00:00:00|2021-03-08 23:59:59|\n",
      "|85707007      |CARTA_PREPAGATA|2021-06-07    |2021-03-13      |2023-08-28 00:00:00|36.49  |+    |true    |ALTRO      |ALTRO          |false |false |entrate_occasionali|versamenti_cash        |null         |false|false |null       |0     |2021-03-13 00:00:00|2021-03-13 23:59:59|\n",
      "|85261821      |EVOLUTION      |2021-07-23    |2021-03-14      |2023-08-28 16:20:56|2.52   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-14 00:00:00|2021-03-14 23:59:59|\n",
      "|87497422      |EVOLUTION      |2021-06-23    |2021-03-18      |2023-08-28 15:58:54|1.91   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-18 00:00:00|2021-03-18 23:59:59|\n",
      "|85261821      |EVOLUTION      |2021-06-09    |2021-03-20      |2023-08-28 08:46:16|1.99   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-20 00:00:00|2021-03-20 23:59:59|\n",
      "|85261821      |EVOLUTION      |2021-06-08    |2021-03-20      |2023-08-28 08:46:16|1.99   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-20 00:00:00|2021-03-20 23:59:59|\n",
      "|85261821      |EVOLUTION      |2021-06-09    |2021-03-20      |2023-08-28 08:46:16|1.0    |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-20 00:00:00|2021-03-20 23:59:59|\n",
      "|85261821      |EVOLUTION      |2021-06-08    |2021-03-20      |2023-08-28 08:46:16|1.98   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-20 00:00:00|2021-03-20 23:59:59|\n",
      "|85261821      |EVOLUTION      |2021-06-08    |2021-03-20      |2023-08-28 08:46:16|0.99   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-20 00:00:00|2021-03-20 23:59:59|\n",
      "|85261821      |EVOLUTION      |2021-06-08    |2021-03-20      |2023-08-28 08:46:16|0.99   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-20 00:00:00|2021-03-20 23:59:59|\n",
      "|85261821      |EVOLUTION      |2021-06-08    |2021-03-20      |2023-08-28 08:46:16|0.99   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-20 00:00:00|2021-03-20 23:59:59|\n",
      "|85261821      |EVOLUTION      |2021-06-08    |2021-03-20      |2023-08-28 08:46:16|3.97   |+    |true    |ALTRO      |ALTRO          |false |false |shopping           |null                   |null         |false|false |ali express|0     |2021-03-20 00:00:00|2021-03-20 23:59:59|\n",
      "+--------------+---------------+--------------+----------------+-------------------+-------+-----+--------+-----------+---------------+------+------+-------------------+-----------------------+-------------+-----+------+-----------+------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TimeBucketing\n",
    "time_bucket_step = TimeBucketing(\n",
    "    time_column_name=DATA_COLUMN_NAME,\n",
    "    time_bucket_size=1,\n",
    "    time_bucket_granularity=\"day\", \n",
    ")\n",
    "time_bucket_df = time_bucket_step(filterd_df, spark)\n",
    "#print(time_bucket_df.count())\n",
    "time_bucket_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 503:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------------+-------------+\n",
      "|ID_CLIENTE_BIC|       bucket_start|         bucket_end|somma_importi|\n",
      "+--------------+-------------------+-------------------+-------------+\n",
      "|      35931425|2021-07-02 00:00:00|2021-07-02 23:59:59|        258.0|\n",
      "|      35931425|2021-07-03 00:00:00|2021-07-03 23:59:59|        108.0|\n",
      "|      35931425|2021-07-04 00:00:00|2021-07-04 23:59:59|         55.0|\n",
      "|      35931425|2021-07-05 00:00:00|2021-07-05 23:59:59|        230.0|\n",
      "|      35931425|2021-07-06 00:00:00|2021-07-06 23:59:59|          0.0|\n",
      "|      35931425|2021-07-10 00:00:00|2021-07-10 23:59:59|        610.0|\n",
      "|      35931425|2021-07-12 00:00:00|2021-07-12 23:59:59|       2458.0|\n",
      "|      35931425|2021-07-16 00:00:00|2021-07-16 23:59:59|         46.0|\n",
      "|      35931425|2021-07-19 00:00:00|2021-07-19 23:59:59|          0.0|\n",
      "|      35931425|2021-07-20 00:00:00|2021-07-20 23:59:59|         14.0|\n",
      "|      35931425|2021-07-23 00:00:00|2021-07-23 23:59:59|         32.0|\n",
      "|      35931425|2021-07-24 00:00:00|2021-07-24 23:59:59|        121.0|\n",
      "|      35931425|2021-07-26 00:00:00|2021-07-26 23:59:59|         13.0|\n",
      "|      35931425|2021-07-30 00:00:00|2021-07-30 23:59:59|         44.0|\n",
      "|      35931425|2021-07-31 00:00:00|2021-07-31 23:59:59|         10.0|\n",
      "|      35931425|2021-08-02 00:00:00|2021-08-02 23:59:59|        270.0|\n",
      "|      35931425|2021-08-03 00:00:00|2021-08-03 23:59:59|        300.0|\n",
      "|      35931425|2021-08-04 00:00:00|2021-08-04 23:59:59|        230.0|\n",
      "|      35931425|2021-08-05 00:00:00|2021-08-05 23:59:59|          0.0|\n",
      "|      35931425|2021-08-11 00:00:00|2021-08-11 23:59:59|        538.0|\n",
      "+--------------+-------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aggregating\n",
    "aggregation_step = Aggregating(\n",
    "    identifier_cols_name=[\"ID_CLIENTE_BIC\"],\n",
    "    time_bucket_cols_name=[\"bucket_start\", \"bucket_end\"],\n",
    "    aggregations=[\n",
    "        Aggregation(\n",
    "            numerical_col_name=\"IMPORTO\",\n",
    "            agg_function=\"sum\",\n",
    "            filters=None,\n",
    "            new_col_name=\"somma_importi\",\n",
    "        ),\n",
    "        \n",
    "    ],\n",
    ")\n",
    "\n",
    "all_aggregations_df = aggregation_step(time_bucket_df, spark)\n",
    "print(all_aggregations_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filling\n",
    "filling_step = Filling(\n",
    "    identifier_cols_name=[\"ID_CLIENTE_BIC\"],\n",
    "    time_bucket_step=time_bucket_step\n",
    ")\n",
    "\n",
    "filled_df = filling_step(df=all_aggregations_df, spark=spark)\n",
    "#print(filled_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/07 12:49:53 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write Time series df\n",
    "# dump to parquet\n",
    "filled_df.write.parquet(path_to_data + \"dataset_timeseries.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1404671"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TimeSeries dataset\n",
    "from ts_train.ts2ft.feature_generating import FeatureGenerating\n",
    "time_series_df = spark.read.parquet(path_to_data + \"filtered_dataset_timeseries.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lowered_time_series_df.show(truncate=False)\n",
    "# Create FeatureGenerating step\n",
    "feature_gen_step = FeatureGenerating(\n",
    "    identifier_col_name=\"ID_CLIENTE_BIC\",\n",
    "    time_col_name = \"bucket_start\",\n",
    "    features_calculators= [\n",
    "        'longest_strike_below_mean',\n",
    "        'benford_correlation',\n",
    "        'cid_ce',\n",
    "        'sum_values',\n",
    "        'standard_deviation',\n",
    "        'number_peaks',\n",
    "        'longest_strike_below_mean',\n",
    "        'abs_energy',\n",
    "        'absolute_sum_of_changes',\n",
    "        'agg_autocorrelation',\n",
    "        'agg_linear_trend',\n",
    "        'augmented_dickey_fuller',\n",
    "        'kurtosis',\n",
    "        'large_standard_deviation',\n",
    "        'mean_second_derivative_central',\n",
    "        'query_similarity_count',\n",
    "        'cid_ce',\n",
    "        'count_above'\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "features_generated_df = feature_gen_step(time_series_df)\n",
    "features_generated_df.write.parquet(path_to_data + \"generated_features.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts_train.ts2ft.feature_generating import FeatureGenerating\n",
    "from ts_train.ts2ft.feature_pruning import FeaturePruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_features_df = spark.read.parquet(path_to_data + \"generated_features_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df = spark.read.parquet(path_to_data + \"targets_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_features_df = generated_features_df.join(targets_df, on=\"ID_CLIENTE_BIC\", how=\"inner\")\n",
    "generated_features_df = generated_features_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop target columns from generated_features_df\n",
    "pandas_feats_df = generated_features_df.drop([\"TARGET\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pandas.Series\n",
    "targets = pd.Series(generated_features_df[\"TARGET\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "2001    0\n",
       "2002    1\n",
       "2003    1\n",
       "2004    0\n",
       "2005    0\n",
       "Length: 2006, dtype: int32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasorrentino/miniconda3/envs/ts_train/lib/python3.11/site-packages/ts_train/ts2ft/feature_pruning.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.insert(0, self.identifier_col_name, identifier_col)\n"
     ]
    }
   ],
   "source": [
    "feature_pruning_step = FeaturePruning(\n",
    "  identifier_col_name=\"ID_CLIENTE_BIC\"\n",
    ")\n",
    "\n",
    "pruned_df, relevance_table = feature_pruning_step(pandas_feats_df, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump relevance_table to file\n",
    "relevance_table.to_csv(path_to_data + \"relevance_table.csv\", index=False)\n",
    "pruned_df.to_csv(path_to_data + \"pruned_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pruned_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_features_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
