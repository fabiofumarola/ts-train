{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/14 15:12:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from ts_train.tr2ts.time_bucketing import TimeBucketing\n",
    "from ts_train.tr2ts.aggregating import Aggregating, Aggregation, Filter, AndGroup\n",
    "from ts_train.tr2ts.filling import Filling\n",
    "import numpy as np \n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from ts_train.ts2ft.feature_generating import FeatureGenerating\n",
    "from ts_train.ts2ft.feature_pruning import FeaturePruning\n",
    "# Codice per visualizzazione su notebook\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "path_to_data = \"../../../dataset_offline/tr2ft/demo_v1/\"\n",
    "DATA_COLUMN_NAME = \"DATA_TRANSAZIONE\"\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"feature_generation\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target prima del downampling\n",
    "target totali: 100746     \n",
    "Target a 1 = 99746     \n",
    "Target a 0 = 1000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df = spark.read.parquet(path_to_data + \"targets_df.parquet\")\n",
    "positive_target = targets_df.filter(targets_df.TARGET == 1)\n",
    "negative_target = targets_df.filter(targets_df.TARGET == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "\n",
    "# Target attuali:\n",
    "Target a 1 = 1000     \n",
    "Target a 0 = 1000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsampling fatto male alla veloce\n",
    "positive_target = targets_df.filter(targets_df.TARGET == 1).limit(1000)\n",
    "negative_target = targets_df.filter(targets_df.TARGET == 0).limit(1000)\n",
    "filtered_target = positive_target.union(negative_target)\n",
    "filtered_target.toPandas().to_parquet(\"target_pandas.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID_CLIENTE_BIC: int, TARGET: int]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transazioni prima del downsampling\n",
    "transazioni negativi: 1000 utenti con 99746 transazioni      \n",
    "transazioni positivi: 99746 utenti con 39682110 transazioni  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tr_df = spark.read.parquet(path_to_data + \"negative_target/sample_transactions.parquet\")\n",
    "positive_tr_df = spark.read.parquet(path_to_data + \"positive_target/100k/100k_user_transactions.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transazioni dopo del downsampling\n",
    "transazioni negativi: 1000 utenti con 99746 transazioni      \n",
    "transazioni positivi: 1000 utenti con 405907 transazioni  \n",
    "totali: 1366311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_id_pos = positive_target.select(\"ID_CLIENTE_BIC\").rdd.flatMap(lambda x: x).collect()\n",
    "filtered_positive_tr_df = positive_tr_df.where(col(\"ID_CLIENTE_BIC\").isin(lista_id_pos))\n",
    "df_tr = filtered_positive_tr_df.union(negative_tr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/14 16:00:50 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/09/14 16:00:52 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "23/09/14 16:00:52 WARN MemoryManager: Total allocation exceeds 95,00% (1.020.054.720 bytes) of heap memory\n",
      "Scaling row group sizes to 95,00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save transactions\n",
    "df_tr.write.format(\"parquet\").save(\"filtered_transaction.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts_train.tr2ts.time_bucketing import TimeBucketing\n",
    "from ts_train.tr2ts.aggregating import Aggregating, Aggregation, Filter, AndGroup, Pivot\n",
    "from ts_train.tr2ts.filling import Filling\n",
    "from pyspark.sql import functions as F\n",
    "from ts_train.ts2ft.feature_generating import FeatureGenerating\n",
    "from ts_train.ts2ft.feature_pruning import FeaturePruning\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "time_bucketing_step = TimeBucketing(\n",
    "  time_col_name=\"DATA_TRANSAZIONE\",\n",
    "  time_bucket_size=1,\n",
    "  time_bucket_granularity=\"week\",\n",
    ")\n",
    "\n",
    "time_bucketed_df = time_bucketing_step(df_tr, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "aggregating_step = Aggregating(\n",
    "  identifier_cols_name=[\"ID_CLIENTE_BIC\"],\n",
    "  time_bucket_cols_name=[\"bucket_start\", \"bucket_end\"],\n",
    "  aggregations=[       \n",
    "    Aggregation(\n",
    "      numerical_col_name=\"IMPORTO\",\n",
    "      agg_function=\"sum\",\n",
    "      filters=[Filter(\"SEGNO\", \"=\", \"-\")],\n",
    "      pivot=Pivot(\"CATEGORY_LIV0\",\"in\",[\n",
    "        'altre_spese',\n",
    "        'tasse',\n",
    "        'investimenti_patrimonio',\n",
    "        'scambio_soldi_tra_privati',\n",
    "      ]),\n",
    "      new_col_name=\"somma_uscite_PIVOTVALUE\",\n",
    "    ),\n",
    "    Aggregation(\n",
    "      numerical_col_name=\"IMPORTO\",\n",
    "      agg_function=\"sum\",\n",
    "      filters=[Filter(\"SEGNO\", \"=\", \"+\")],\n",
    "      pivot=Pivot(\"CATEGORY_LIV0\",\"in\",[\n",
    "        'entrate_occasionali',\n",
    "        'entrate_regolari',\n",
    "        'investimenti_patrimonio',\n",
    "        'scambio_soldi_tra_privati',\n",
    "      ]),\n",
    "      new_col_name=\"somma_entrate_PIVOTVALUE\",\n",
    "    ),\n",
    "  ],\n",
    ")\n",
    "\n",
    "aggregated_df = aggregating_step(time_bucketed_df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filling_step = Filling(\n",
    "  identifier_cols_name=[\"ID_CLIENTE_BIC\"],\n",
    "  time_bucket_step=time_bucketing_step\n",
    ")\n",
    "\n",
    "filled_df = filling_step(df=aggregated_df, spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filled_df.write.format(\"parquet\").save(\"filtered_timeseries.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ts_train.ts2ft.feature_generating import FeatureGenerating\n",
    "\n",
    "feature_generating_step = FeatureGenerating(\n",
    "  identifier_col_name=\"ID_CLIENTE_BIC\",\n",
    "  time_col_name = \"bucket_start\",\n",
    "  feature_calculators= [\n",
    "    'minimum',\n",
    "    'c3', \n",
    "    'last_location_of_maximum',\n",
    "    'last_location_of_minimum',\n",
    "    'longest_strike_below_mean',\n",
    "    'median',\n",
    "    'variance', \n",
    "    'kurtosis', \n",
    "    'number_peaks', \n",
    "    'linear_trend',\n",
    "    'ar_coefficient',\n",
    "  ]\n",
    ")\n",
    "\n",
    "# drop bucket end \n",
    "timeseries_df = filled_df.drop(\"bucket_end\")\n",
    "\n",
    "features_generated_df = feature_generating_step(timeseries_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_generated_df = features_generated_df.join(targets_df, on=\"ID_CLIENTE_BIC\", how=\"inner\")\n",
    "time_series_df = features_generated_df.toPandas()\n",
    "\n",
    "# drop target columns from time_series_df\n",
    "targets = pd.Series(time_series_df[\"TARGET\"].values)\n",
    "pandas_feats_df = time_series_df.drop([\"TARGET\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pruning_step = FeaturePruning(\n",
    "  identifier_col_name=\"ID_CLIENTE_BIC\"\n",
    ")\n",
    "\n",
    "pruned_df, relevance_table = feature_pruning_step(pandas_feats_df, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generated features: 130\n",
      "Dropped 51 features: {'somma_entrate_entrate_regolari__c3__lag_2', 'somma_uscite_tasse__c3__lag_3', 'TARGET', 'somma_uscite_tasse__number_peaks__n_10', 'somma_uscite_investimenti_patrimonio__last_location_of_minimum', 'somma_uscite_tasse__ar_coefficient__coeff_10__k_10', 'somma_entrate_entrate_regolari__c3__lag_3', 'somma_entrate_investimenti_patrimonio__last_location_of_minimum', 'somma_entrate_entrate_regolari__median', 'somma_entrate_entrate_occasionali__number_peaks__n_50', 'somma_uscite_tasse__c3__lag_2', 'somma_entrate_investimenti_patrimonio__c3__lag_3', 'somma_uscite_investimenti_patrimonio__c3__lag_2', 'somma_uscite_scambio_soldi_tra_privati__number_peaks__n_50', 'somma_uscite_tasse__number_peaks__n_50', 'somma_entrate_entrate_regolari__linear_trend__attr_\"rvalue\"', 'somma_uscite_tasse__last_location_of_maximum', 'somma_uscite_investimenti_patrimonio__number_peaks__n_50', 'somma_uscite_altre_spese__minimum', 'somma_uscite_investimenti_patrimonio__number_peaks__n_10', 'somma_uscite_tasse__number_peaks__n_3', 'somma_entrate_entrate_regolari__c3__lag_1', 'somma_entrate_scambio_soldi_tra_privati__minimum', 'somma_entrate_entrate_occasionali__minimum', 'somma_uscite_investimenti_patrimonio__longest_strike_below_mean', 'somma_uscite_tasse__minimum', 'somma_uscite_investimenti_patrimonio__minimum', 'somma_entrate_entrate_regolari__minimum', 'somma_entrate_entrate_occasionali__linear_trend__attr_\"rvalue\"', 'somma_uscite_scambio_soldi_tra_privati__minimum', 'somma_entrate_investimenti_patrimonio__minimum', 'somma_uscite_tasse__variance', 'somma_uscite_altre_spese__linear_trend__attr_\"rvalue\"', 'somma_entrate_scambio_soldi_tra_privati__number_peaks__n_50', 'somma_uscite_investimenti_patrimonio__c3__lag_3', 'somma_entrate_investimenti_patrimonio__median', 'somma_entrate_investimenti_patrimonio__c3__lag_2', 'somma_uscite_investimenti_patrimonio__number_peaks__n_3', 'somma_uscite_tasse__last_location_of_minimum', 'somma_uscite_investimenti_patrimonio__linear_trend__attr_\"rvalue\"', 'somma_uscite_tasse__median', 'somma_uscite_investimenti_patrimonio__last_location_of_maximum', 'somma_uscite_investimenti_patrimonio__ar_coefficient__coeff_10__k_10', 'somma_uscite_investimenti_patrimonio__c3__lag_1', 'somma_entrate_investimenti_patrimonio__linear_trend__attr_\"rvalue\"', 'somma_entrate_investimenti_patrimonio__number_peaks__n_50', 'somma_uscite_investimenti_patrimonio__number_peaks__n_1', 'somma_uscite_investimenti_patrimonio__median', 'somma_entrate_entrate_regolari__longest_strike_below_mean', 'somma_uscite_investimenti_patrimonio__variance', 'somma_uscite_tasse__c3__lag_1'}\n",
      "Total final features: 79\n"
     ]
    }
   ],
   "source": [
    "dropped_features = set(time_series_df.columns) - set(pruned_df.columns)\n",
    "# original features\n",
    "print(f\"Total generated features: {len(time_series_df.columns)}\")\n",
    "print(f\"Dropped {len(dropped_features)} features: {dropped_features}\")\n",
    "print(f\"Total final features: {len(pruned_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_df.to_parquet(\"filtered_features.parquet\", index=False)  # Set index=False to exclude the DataFrame index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df_pandas = targets_df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
